# ğŸ§  Ollama-GPT â€” Self-Hosted Private LLM with Open WebUI

Run your own **ChatGPT-like interface** entirely offline using:
- **[Ollama](https://ollama.com/)** â€” Run AI models locally on your own machine.
- **[Open WebUI](https://github.com/open-webui/open-webui)** â€” Beautiful ChatGPT-style interface.

> âš¡ **Privacy-first**: No API keys, no data sharing â€” 100% offline.

---

## ğŸ“¦ Features
- ğŸš€ Run open-source LLMs locally (LLaMA, CodeLLaMA, Qwen, Phi, etc.)
- ğŸ”’ Fully private â€” your data stays on your machine
- ğŸ³ Docker-based setup for quick deployment
- ğŸ’» CLI or Web UI access
- âš™ Configurable CPU & RAM limits

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/ashubambal/ollama-gpt.git
cd ollama-gpt
```

### 2ï¸âƒ£ Configure Environment Variables
Edit docker-compose.yml and set your own WEBUI_SECRET_KEY:
```bash
WEBUI_SECRET_KEY=enter-your-secret-key
```

### 3ï¸âƒ£ Start the Containers
```bash
docker compose up -d
```

### 4ï¸âƒ£ Verify Running Containers
```bash
docker ps
```
Expected:

- ollama â€” AI model server
- open-webui â€” Web interface


### 5ï¸âƒ£ Install a Model in Ollama
- Enter the Ollama container:
```bash
docker exec -it ollama bash
```

- Pull a model (example: llama3.2:1b-instruct-q4_0):
```bash
ollama pull llama3.2:1b-instruct-q4_0
```

- Test in CLI:
```bash
ollama run llama3.2:1b-instruct-q4_0
```

### 6ï¸âƒ£ Access via Web Interface
- Open browser â†’ http://localhost:8080
- Login using WEBUI_SECRET_KEY
- Select your model and start chatting ğŸ‰

### âš™ï¸ Docker Compose Configuration
```bash
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_LLM_MAX_MEMORY=6000MB
      - OLLAMA_NUM_THREADS=6
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=1
    deploy:
      resources:
        limits:
          cpus: '6.0'
        reservations:
          cpus: '2.0'

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=enter-key-here
    depends_on:
      - ollama

volumes:
  ollama_data:
  open_webui_data:
```

### ğŸ“š Model Resources
- Ollama Model Library
- Recommended code-focused models:
1.codellama
2.qwen2.5-coder
3.phi3.5

### ğŸ¥ Output you can see in below video
Link it in README:
<video src="assets/demo.mp4" controls width="700"></video>


### ğŸ›  Stopping the Services
```bash
docker compose down
```

### ğŸ“„ License
MIT â€” Free to use and modify.
```bash

---

This is **one continuous README** â€” no separated instructions, everything from install to demo is inside.  

If you want, I can now **add screenshots of the Open WebUI UI** directly into this same README so that new users see exactly what theyâ€™ll get before running it.  

Do you want me to add those screenshots in this file?

```

